{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CSE545-SDG3-Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.types import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(appName=\"SDG3\")\n",
    "sqlCtx = SQLContext(sc)\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = r'..\\..\\..\\LinkPE14US'\n",
    "df = sqlCtx.read.text(os.path.join(data_path, \"VS14LINK.USNUMPUB\"))  #350 mb\n",
    "#df = sqlCtx.read.text(\"/VS14LINK.USDENPUB\") # 5gb\n",
    "#df = sqlCtx.read.text(\"/*.*PUB\") # both\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blank_as_null(x):\n",
    "    return when(col(x) != ' ', col(x)).otherwise(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pandas data frame after adding column metadata\n",
    "pndf = df.select(\n",
    "    df.value.substr(9,4).cast(IntegerType()).alias('Birth_Year'),\n",
    "    df.value.substr(13,2).cast(IntegerType()).alias('Birth_Month'),\n",
    "    df.value.substr(75,2).cast(IntegerType()).alias('Mothers_Age'),\n",
    "    #df.value.substr(299,3).cast(IntegerType()).alias('Delivery_Weight_lbs'),\n",
    "    df.value.substr(492,2).cast(IntegerType()).alias('Gestational_Age_weeks'),\n",
    "    df.value.substr(332,2).cast(IntegerType()).alias('Num_Prev_Cesareans'),\n",
    "    df.value.substr(454,1).cast(IntegerType()).alias('Plurality'),\n",
    "    df.value.substr(568,1).cast(StringType()).alias('Infant_Living')   \n",
    "    \n",
    ").withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "pndf.printSchema\n",
    "pndf = pndf.na.fill({'Num_Prev_Cesareans': 0.0})\n",
    "#pndf = pndf.withColumn(\"Infant_Living\", blank_as_null(\"Infant_Living\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Birth_Year=2013, Birth_Month=7, Mothers_Age=24, Gestational_Age_weeks=10, Num_Prev_Cesareans=0, Plurality=1, Infant_Living='Y', id=0, Infant_Living_Index=0.0),\n",
       " Row(Birth_Year=2013, Birth_Month=9, Mothers_Age=27, Gestational_Age_weeks=8, Num_Prev_Cesareans=1, Plurality=1, Infant_Living='Y', id=1, Infant_Living_Index=0.0)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCol=\"Infant_Living\", outputCol='Infant_Living_Index')\n",
    "pndf = stringIndexer.fit(pndf).transform(pndf)\n",
    "pndf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Birth_Year=2013, Birth_Month=7, Mothers_Age=24, Gestational_Age_weeks=10, Num_Prev_Cesareans=0, Plurality=1, Infant_Living='Y', id=0, Infant_Living_Index=0.0, features=DenseVector([2013.0, 7.0, 24.0, 10.0, 0.0, 1.0, 0.0, 0.0]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "pca_cols = pndf.columns\n",
    "pca_cols.remove('Infant_Living')\n",
    "assembler = VectorAssembler(inputCols=pca_cols, outputCol='features')\n",
    "vector_df = assembler.transform(pndf)\n",
    "vector_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Birth_Year=2013, Birth_Month=7, Mothers_Age=24, Gestational_Age_weeks=10, Num_Prev_Cesareans=0, Plurality=1, Infant_Living='Y', id=0, Infant_Living_Index=0.0, features=DenseVector([2013.0, 7.0, 24.0, 10.0, 0.0, 1.0, 0.0, 0.0]), scaledFeatures=DenseVector([6292.0563, 2.0564, 3.7862, 1.0951, 0.0, 2.3585, 0.0, 0.0]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol='features', \n",
    "                        outputCol='scaledFeatures', withMean=False, withStd=True) # TODO: should withMean be True??\n",
    "scaled_df = scaler.fit(vector_df).transform(vector_df)\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Birth_Year=2013, Birth_Month=7, Mothers_Age=24, Gestational_Age_weeks=10, Num_Prev_Cesareans=0, Plurality=1, Infant_Living='Y', id=0, Infant_Living_Index=0.0, features=DenseVector([2013.0, 7.0, 24.0, 10.0, 0.0, 1.0, 0.0, 0.0]), scaledFeatures=DenseVector([6292.0563, 2.0564, 3.7862, 1.0951, 0.0, 2.3585, 0.0, 0.0]), normalizeFeatures=DenseVector([0.9985, 0.0003, 0.0006, 0.0002, 0.0, 0.0004, 0.0, 0.0]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "nrmlzer = Normalizer(inputCol='scaledFeatures', outputCol='normalizeFeatures', p=1.0)\n",
    "l1Normalized = nrmlzer.transform(scaled_df)\n",
    "l1Normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------+\n",
      "|features_pca                                                                          |\n",
      "+--------------------------------------------------------------------------------------+\n",
      "|[-0.9330562507431365,0.023164943780604293,-0.008915042901885535,0.0028879196561173424]|\n",
      "|[-0.9328930191839417,0.023144873923885843,-0.008988075307957161,0.0028732940407907126]|\n",
      "|[-0.9329377333898223,0.023129934209772694,-0.009020190428083955,0.0028982097299626165]|\n",
      "|[-0.9329831027945481,0.02313465819558465,-0.00902360010932929,0.00292039268416863]    |\n",
      "|[-0.9327297017248617,0.023151478480475444,-0.009040170681808157,0.002788953083242807] |\n",
      "|[-0.9327623302599205,0.023116118200052542,-0.00905490911348384,0.0028330927845812842] |\n",
      "|[-0.9330517371405233,0.02308142638488892,-0.009023234205438425,0.0029859167109833303] |\n",
      "|[-0.9328137213676434,0.023125917049374483,-0.009106676829105194,0.002878866779627327] |\n",
      "|[-0.9329216611863506,0.023072337961873093,-0.009061367063794174,0.0029429611956703463]|\n",
      "|[-0.9330129165903325,0.02311784851876486,-0.009097915536576845,0.002981298056319692]  |\n",
      "|[-0.9327959910575667,0.023152875940388597,-0.009129087390214508,0.002865169119423943] |\n",
      "|[-0.9323390063097473,0.02328145998128586,-0.00924593447731793,0.002918123540890484]   |\n",
      "|[-0.93289589575802,0.023419202875570738,-0.008924404354378626,0.003049938077194639]   |\n",
      "|[-0.9327658553435632,0.02341007450509788,-0.008962544380784972,0.0030069802443570015] |\n",
      "|[-0.9327559605268486,0.023351431895876212,-0.008914313448351988,0.003012195643419768] |\n",
      "|[-0.9326386744455014,0.023382395802671492,-0.008941120927918055,0.0029472869041822737]|\n",
      "|[-0.932775235233316,0.023432657834402598,-0.008981057198037134,0.003007797379238249]  |\n",
      "|[-0.9323008280451935,0.023394763211338602,-0.008999230130949167,0.002795136334798226] |\n",
      "|[-0.9323779550671296,0.023521420461946496,-0.00910880656508445,0.003119336073760485]  |\n",
      "|[-0.9328620920587775,0.02338856004064624,-0.008987996128403678,0.0030809837437176177] |\n",
      "+--------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "num_principal_comp = 4\n",
    "pca = PCA(k=num_principal_comp, inputCol='normalizeFeatures', outputCol='features_pca')\n",
    "pca_model = pca.fit(l1Normalized)\n",
    "pca_feat = pca_model.transform(l1Normalized).select('features_pca')\n",
    "pca_feat.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
